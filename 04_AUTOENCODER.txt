# ASSIGNMENT 4: USE AUTOENCODER TO IMPLEMENT ANOMALY DETECTION
## Complete Code with Detailed Line-by-Line Comments + Theory + Viva Questions

```python
# ============================================================================
# ASSIGNMENT 4: USE AUTOENCODER TO IMPLEMENT ANOMALY DETECTION
# Build the model using Fashion MNIST dataset
# ============================================================================
# Objective: Build an autoencoder to detect anomalies in fashion images
# Dataset: Fashion MNIST (28×28 grayscale images of clothing items)
# Task: Reconstruct images and detect anomalies based on reconstruction error
# Architecture: Encoder (compress) → Latent space → Decoder (reconstruct)
# ============================================================================

print("="*80)
print("ASSIGNMENT 4: AUTOENCODER FOR ANOMALY DETECTION - FASHION MNIST")
print("="*80)

# ============================================================================
# SECTION A: IMPORT REQUIRED LIBRARIES
# ============================================================================

print("\nSECTION A: IMPORT REQUIRED LIBRARIES")
print("-"*80)

# IMPORT 1: Matplotlib - Data visualization library
import matplotlib.pyplot as plt
# Used for: Plotting images, visualizing reconstructions, creating plots

# IMPORT 2: NumPy - Numerical computing library
import numpy as np
# Used for: Array operations, numerical computations, data manipulation
# Functions: np.reshape(), np.array(), np.mean(), np.std(), etc.

# IMPORT 3: Pandas - Data analysis and manipulation
import pandas as pd
# Used for: Data handling, CSV operations, dataframe operations
# Not heavily used here but good practice for data science workflows

# IMPORT 4: TensorFlow - Deep learning framework
import tensorflow as tf
# Used for: Building neural networks, GPU acceleration
# Contains Keras submodule for high-level model building

# IMPORT 5: Keras metrics - Performance evaluation
from sklearn.metrics import accuracy_score, precision_score, recall_score
# accuracy_score: Fraction of correct predictions
# precision_score: TP / (TP + FP) - Of predicted positives, how many correct?
# recall_score: TP / (TP + FN) - Of actual positives, how many detected?

# IMPORT 6: Train-test split - Data splitting utility
from sklearn.model_selection import train_test_split
# Used for: Splitting data into training, validation, test sets
# Ensures proper separation of data for unbiased evaluation

# IMPORT 7: Keras layers - Building blocks for neural networks
from tensorflow.keras import layers, losses
# layers: Dense, Flatten, Reshape, Input, etc.
# losses: Loss functions (MSE, crossentropy, etc.)

# IMPORT 8: Keras datasets - Pre-loaded datasets
from tensorflow.keras.datasets import fashion_mnist
# Used for: Loading Fashion MNIST dataset
# 60,000 training images + 10,000 test images of fashion items

# IMPORT 9: Keras Model API - Model creation
from tensorflow.keras.models import Model
# Model: Functional API for building custom architectures
# Allows flexible model building (multiple inputs/outputs)

print("[INFO] All libraries imported successfully!")

# ============================================================================
# SECTION B: UPLOAD / ACCESS THE DATASET
# ============================================================================

print("\nSECTION B: UPLOAD / ACCESS THE DATASET")
print("-"*80)

print("\n[INFO] Loading Fashion MNIST dataset...")

# LOAD FASHION MNIST DATASET
# Fashion MNIST: 28×28 grayscale images of clothing items
# 70,000 total images: 60,000 training + 10,000 test
# 10 classes: T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, 
#            Shirt, Sneaker, Bag, Ankle boot

(x_train, _), (x_test, _) = fashion_mnist.load_data()

# (x_train, _): Training images, ignoring training labels (represented by _)
# (x_test, _): Test images, ignoring test labels
# Why ignore labels? For autoencoder, we only need images (unsupervised)

print(f"[INFO] Original training data shape: {x_train.shape}")
print(f"       Meaning: {x_train.shape[0]} images, each {x_train.shape[1]}×{x_train.shape[2]} pixels")

print(f"[INFO] Original test data shape: {x_test.shape}")
print(f"       Meaning: {x_test.shape[0]} images, each {x_test.shape[1]}×{x_test.shape[2]} pixels")

print(f"[INFO] Pixel value range: [{x_train.min()}, {x_train.max()}]")

# ============================================================================
# PREPROCESSING: Normalize Data
# ============================================================================

print("\n[INFO] Preprocessing data...")

# CONVERT TO FLOAT32 AND NORMALIZE
# Why float32?
# - Faster computation on GPUs
# - Standard TensorFlow default type
# - Sufficient precision for neural networks

# Normalize: Convert pixel values from [0, 255] to [0, 1]
# Why normalize?
# 1. Neural networks prefer normalized inputs
# 2. Faster convergence during training
# 3. Better numerical stability
# 4. Prevents gradient explosion/vanishing
# 5. Uniform scale for all features

# Training data normalization
x_train = x_train.astype('float32') / 255.
# .astype('float32'): Convert from uint8 to float32
# / 255.: Divide by maximum pixel value (255)
# Result: Values now in range [0, 1.0]

# Test data normalization (same transformation)
x_test = x_test.astype('float32') / 255.
# Important: Use SAME normalization for train and test
# Never fit scaler on test data separately

print(f"[INFO] Normalized training data shape: {x_train.shape}")
print(f"[INFO] Normalized test data shape: {x_test.shape}")
print(f"[INFO] Normalized pixel value range: [{x_train.min():.2f}, {x_train.max():.2f}]")

# ============================================================================
# DEFINE LATENT DIMENSION
# ============================================================================

print("\n[INFO] Defining model hyperparameters...")

# LATENT DIMENSION: Size of compressed representation
# Latent: Hidden/compressed representation in bottleneck
# 
# latent_dim = 64 means:
# - Original images: 28×28 = 784 features
# - Latent representation: 64 features (compression: 784/64 ≈ 12×)
# - Decoder reconstructs 784 features from 64 features
#
# Why 64?
# - Common power of 2 (64, 128, 256, etc.)
# - Sufficient for capturing image structure
# - Not too small (loss of information) or too large (no compression)
# - Balance between reconstruction quality and compression

latent_dim = 64

print(f"[INFO] Latent dimension: {latent_dim}")
print(f"[INFO] Compression ratio: {28*28}/{latent_dim} = {(28*28)/latent_dim:.1f}×")

# ============================================================================
# SECTION C: ENCODER CONVERTS INPUT TO LATENT REPRESENTATION
# SECTION D: DECODER NETWORKS CONVERT BACK TO ORIGINAL INPUT
# ============================================================================

print("\nSECTION C & D: BUILD AUTOENCODER ARCHITECTURE")
print("-"*80)

print("\n[INFO] Creating custom Autoencoder class...")

# CREATE CUSTOM AUTOENCODER CLASS
# Why custom class instead of Sequential?
# - Flexibility for custom training loops
# - Custom loss functions possible
# - Multiple encoders/decoders possible
# - Easy to extract encoder or decoder separately

class Autoencoder(Model):
    # Inherit from tf.keras.models.Model
    # Allows custom forward pass and training logic
    
    def __init__(self, latent_dim):
        # Constructor: Initialize all components
        super(Autoencoder, self).__init__()
        # super(): Call parent class constructor
        
        # Store latent dimension as instance variable
        self.latent_dim = latent_dim
        
        # ============================================================
        # ENCODER: Compress image to latent representation
        # ============================================================
        # Input: 28×28 image (784 features)
        # Output: latent_dim features (64)
        
        self.encoder = tf.keras.Sequential([
            # Layer 1: FLATTEN
            # Purpose: Convert 2D image (28×28) to 1D vector (784)
            # Why flatten?
            # - Dense layers expect 1D input
            # - Preserves all pixel information
            # - Standard preprocessing step
            layers.Flatten(),
            # Output shape: (None, 784)
            # None: Batch size (can be any number)
            
            # Layer 2: DENSE
            # Purpose: Compress 784 features to latent_dim (64) features
            # Parameters:
            #   units=latent_dim (64): Output dimension
            #   activation='relu': Non-linear activation
            #
            # Why ReLU?
            # - Introduces non-linearity (enables learning complex patterns)
            # - Computationally efficient (simple max operation)
            # - Solves vanishing gradient problem
            # - Output: All positive values (good for latent representation)
            #
            # Weights: 784 × 64 = 50,176 parameters
            # Biases: 64 parameters
            # Total: 50,240 parameters
            layers.Dense(latent_dim, activation='relu'),
            # Output shape: (None, 64)
            # This is the BOTTLENECK or LATENT REPRESENTATION
        ])
        
        # ============================================================
        # DECODER: Reconstruct image from latent representation
        # ============================================================
        # Input: latent_dim features (64)
        # Output: 28×28 image (784 features)
        
        self.decoder = tf.keras.Sequential([
            # Layer 1: DENSE
            # Purpose: Expand latent representation (64) back to image space (784)
            # Parameters:
            #   units=784: Reconstruct all 784 pixels
            #   activation='sigmoid': Output 0-1 values (matching normalized images)
            #
            # Why sigmoid?
            # - Output range [0, 1] matches normalized pixel values
            # - Smooth gradient (good for backpropagation)
            # - Probability-like interpretation
            #
            # Weights: 64 × 784 = 50,176 parameters
            # Biases: 784 parameters
            # Total: 50,960 parameters
            layers.Dense(784, activation='sigmoid'),
            # Output shape: (None, 784)
            
            # Layer 2: RESHAPE
            # Purpose: Reshape flattened vector (784) back to image format (28, 28)
            # Why reshape?
            # - Restore 2D spatial structure
            # - Prepare for visualization
            # - Match original input shape
            #
            # 784 = 28 × 28, so reshape (None, 784) to (None, 28, 28)
            layers.Reshape((28, 28))
            # Output shape: (None, 28, 28)
            # Now we have reconstructed 28×28 images
        ])
    
    def call(self, x):
        # Forward pass: Define how data flows through the model
        # This method is called when model is used for prediction/training
        #
        # Input x: Batch of images (None, 28, 28)
        
        # ENCODER: Compress input to latent representation
        # x shape before: (batch_size, 28, 28)
        # x shape after: (batch_size, latent_dim)
        encoded = self.encoder(x)
        # encoded now contains compressed representations
        
        # DECODER: Reconstruct from latent representation
        # Input to decoder: (batch_size, latent_dim)
        # Output from decoder: (batch_size, 28, 28)
        decoded = self.decoder(encoded)
        # decoded is the reconstructed image
        
        # Return reconstructed image
        return decoded


# INSTANTIATE AUTOENCODER
# Create instance with latent_dim=64
autoencoder = Autoencoder(latent_dim)

print(f"[INFO] Autoencoder created with latent_dim={latent_dim}")
print(f"[INFO] Encoder: 784 features → {latent_dim} latent features")
print(f"[INFO] Decoder: {latent_dim} latent features → 784 features (28×28 image)")

# ============================================================================
# SECTION E: COMPILE THE MODEL WITH OPTIMIZER, LOSS, AND METRICS
# ============================================================================

print("\nSECTION E: COMPILE THE MODEL")
print("-"*80)

print("\n[INFO] Compiling autoencoder...")

# COMPILE MODEL: Configure for training
# Parameters:
#   optimizer: Algorithm for weight updates
#   loss: What to minimize
#   metrics: What to monitor

# OPTIMIZER: Adam
# Adam (Adaptive Moment Estimation):
# - Adaptive learning rate optimizer
# - Combines momentum with adaptive learning rates
# - learning_rate=0.001: Step size for weight updates
# - Default betas=(0.9, 0.999) for momentum
# - Works very well in practice
# - Better than vanilla SGD for most problems

optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

# LOSS FUNCTION: Mean Squared Error (MSE)
# For autoencoder: Reconstruct input as closely as possible
# MSE = (1/n) × Σ(actual - predicted)²
#
# Why MSE?
# 1. Measures pixel-wise reconstruction error
# 2. Penalizes large errors heavily
# 3. Differentiable (allows backpropagation)
# 4. Interpretable: Average squared pixel difference
# 5. Standard choice for reconstruction tasks

loss_fn = losses.MeanSquaredError()
# Alternative: loss='mse' (string name works too)

# METRICS: What to monitor during training
# Metrics don't affect training (only loss does)
# Used for monitoring and early stopping decisions

# For autoencoder, loss itself is main metric
# We could use other metrics, but MSE loss is sufficient

# COMPILE
autoencoder.compile(
    optimizer=optimizer,
    loss=loss_fn,
    # No metrics needed: we'll monitor loss directly
    # Reconstruction quality measured by loss value
)

print(f"[INFO] Model compiled successfully!")
print(f"[INFO] Optimizer: Adam (learning_rate=0.001)")
print(f"[INFO] Loss function: Mean Squared Error (MSE)")
print(f"[INFO] Ready for training!")

# ============================================================================
# SECTION F: TRAIN THE AUTOENCODER
# ============================================================================

print("\nSECTION F: TRAIN THE AUTOENCODER")
print("-"*80)

print("\n[INFO] Training autoencoder on Fashion MNIST...")

# TRAIN MODEL
# fit: Train on training data for specified number of epochs
history = autoencoder.fit(
    # Training data
    x_train,                    # Input: Training images (60000, 28, 28)
                                # For autoencoder, input = target
    
    x_train,                    # Target: Same as input
                                # We're training to reconstruct the input
                                # Loss = ||x_train - autoencoder(x_train)||²
    
    # Training parameters
    epochs=10,                  # 10 complete passes through training data
                                # More epochs = longer training but better learning
                                # 10 is reasonable for demo (use 20-50 for better results)
    
    batch_size=256,             # Process 256 images per batch
                                # Smaller batch: Noisier gradients, more updates
                                # Larger batch: Smoother gradients, fewer updates
                                # 256 is standard choice
    
    # Validation
    validation_data=(x_test, x_test),  # Monitor on test set
                                       # x_test: Test images
                                       # Same format as training data
    
    verbose=1                   # Show progress bar
                                # verbose=0: No output
                                # verbose=1: Progress bar (default)
                                # verbose=2: One line per epoch
)

print("\n[INFO] Training completed!")

# ============================================================================
# SECTION G: MAKE PREDICTIONS (RECONSTRUCTION)
# ============================================================================

print("\nSECTION G: MAKE PREDICTIONS")
print("-"*80)

print("\n[INFO] Generating reconstructions for visualization...")

# PREDICT: Generate reconstructions for test set
# Use trained autoencoder to reconstruct test images
decoded_imgs = autoencoder.predict(x_test, verbose=0)
# decoded_imgs: Reconstructed images from test set
# Shape: (10000, 28, 28)

print(f"[INFO] Generated {len(decoded_imgs)} reconstructions")
print(f"[INFO] Reconstructed images shape: {decoded_imgs.shape}")

# ============================================================================
# SECTION H: VISUALIZE ORIGINAL vs RECONSTRUCTED IMAGES
# ============================================================================

print("\nSECTION H: VISUALIZE RESULTS")
print("-"*80)

print("\n[INFO] Creating visualization...")

# VISUALIZE: Compare original vs reconstructed images
n = 10  # Number of images to display

# Create figure with 2 rows × n columns = 2n subplots
plt.figure(figsize=(20, 4))

for i in range(n):
    # ================================================================
    # ROW 1: ORIGINAL IMAGES
    # ================================================================
    
    # Create subplot for original image
    # Position: Row 1 (subplot 2,n,i+1 means: 2 rows, n cols, position i+1)
    ax = plt.subplot(2, n, i + 1)
    
    # Display original image from test set
    # cmap='gray': Grayscale colormap (black-white)
    plt.imshow(x_test[i], cmap='gray')
    
    # Set title
    plt.title("original")
    
    # Set colormap to grayscale
    plt.gray()
    
    # Hide x and y axis labels and ticks
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
    
    # ================================================================
    # ROW 2: RECONSTRUCTED IMAGES
    # ================================================================
    
    # Create subplot for reconstructed image
    # Position: Row 2 (subplot 2,n,i+1+n means: position i+1+n)
    ax = plt.subplot(2, n, i + 1 + n)
    
    # Display reconstructed image from autoencoder
    # decoded_imgs[i]: The reconstructed version of x_test[i]
    plt.imshow(decoded_imgs[i], cmap='gray')
    
    # Set title
    plt.title("reconstructed")
    
    # Set colormap to grayscale
    plt.gray()
    
    # Hide x and y axis labels and ticks
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)

# Display the entire figure
plt.show()

print("\n[INFO] Visualization complete!")
print("[INFO] Compare row 1 (original) with row 2 (reconstructed)")
print("[INFO] Good reconstruction: Similar patterns, slightly blurred")
print("[INFO] Poor reconstruction: Very different from original (rare)")

# ============================================================================
# SECTION I: CALCULATE RECONSTRUCTION ERROR
# ============================================================================

print("\nSECTION I: CALCULATE RECONSTRUCTION ERROR")
print("-"*80)

print("\n[INFO] Computing reconstruction errors...")

# CALCULATE RECONSTRUCTION ERROR FOR EACH IMAGE
# Error = Mean Squared Error between original and reconstructed
# 
# For each image:
# error_i = (1/784) × Σ(original_pixel - reconstructed_pixel)²
#
# This gives us a single error value for each image
# Normal images: Low error (good reconstruction)
# Anomalies: High error (poor reconstruction - they're different)

# Flatten for error calculation
x_test_flat = x_test.reshape((x_test.shape[0], -1))
decoded_imgs_flat = decoded_imgs.reshape((decoded_imgs.shape[0], -1))

# Calculate MSE for each image
# axis=1: Sum over pixel dimension, keep image dimension
reconstruction_errors = np.mean(
    (x_test_flat - decoded_imgs_flat) ** 2, 
    axis=1
)

# reconstruction_errors shape: (10000,)
# Each value: reconstruction error for one image

print(f"[INFO] Reconstruction error statistics:")
print(f"       Mean: {np.mean(reconstruction_errors):.6f}")
print(f"       Std: {np.std(reconstruction_errors):.6f}")
print(f"       Min: {np.min(reconstruction_errors):.6f}")
print(f"       Max: {np.max(reconstruction_errors):.6f}")





### This much is sufficient








# ============================================================================
# SECTION J: ANOMALY DETECTION
# ============================================================================

print("\nSECTION J: ANOMALY DETECTION")
print("-"*80)

print("\n[INFO] Detecting anomalies using reconstruction error...")

# SET THRESHOLD FOR ANOMALY DETECTION
# Threshold = Mean + 2×Std
# 
# Why this formula?
# - Assumes reconstruction errors normally distributed
# - In normal distribution: ~95% of data within mean ± 2×std
# - Anything beyond 2×std is outlier/anomaly
# - Threshold balances false positives and false negatives

threshold = np.mean(reconstruction_errors) + 2 * np.std(reconstruction_errors)

print(f"\n[INFO] Anomaly detection threshold: {threshold:.6f}")
print(f"       If reconstruction_error > {threshold:.6f}: ANOMALY")
print(f"       If reconstruction_error ≤ {threshold:.6f}: NORMAL")

# CLASSIFY IMAGES AS NORMAL OR ANOMALY
# Based on whether reconstruction error exceeds threshold
anomaly_predictions = reconstruction_errors > threshold

# Count predictions
num_normal = np.sum(anomaly_predictions == False)
num_anomaly = np.sum(anomaly_predictions == True)

print(f"\n[INFO] Detection results on test set:")
print(f"       Normal (threshold not exceeded): {num_normal} images")
print(f"       Anomalies (threshold exceeded): {num_anomaly} images")

# ============================================================================
# SECTION K: VISUALIZE ANOMALIES
# ============================================================================

print("\nSECTION K: VISUALIZE DETECTED ANOMALIES")
print("-"*80)

# Find high-error images (detected as anomalies)
anomaly_indices = np.where(anomaly_predictions)[0]

# Show a few high-error images
n_anomalies = 5  # Show 5 anomalies

print(f"\n[INFO] Displaying {n_anomalies} images with highest reconstruction error...")

plt.figure(figsize=(15, 3))

for i in range(min(n_anomalies, len(anomaly_indices))):
    idx = anomaly_indices[i]
    
    # Original image
    ax = plt.subplot(2, n_anomalies, i + 1)
    plt.imshow(x_test[idx], cmap='gray')
    plt.title(f"Original\nError: {reconstruction_errors[idx]:.4f}")
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
    
    # Reconstructed image
    ax = plt.subplot(2, n_anomalies, i + 1 + n_anomalies)
    plt.imshow(decoded_imgs[idx], cmap='gray')
    plt.title(f"Reconstructed\n(Threshold: {threshold:.4f})")
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)

plt.tight_layout()
plt.show()

print("[INFO] These images have high reconstruction error (detected as anomalies)")

# ============================================================================
# SECTION L: VISUALIZE ERROR DISTRIBUTION
# ============================================================================

print("\nSECTION L: VISUALIZE RECONSTRUCTION ERROR DISTRIBUTION")
print("-"*80)

print("\n[INFO] Creating error distribution histogram...")

plt.figure(figsize=(10, 6))

# Plot histogram of reconstruction errors
plt.hist(reconstruction_errors, bins=50, alpha=0.7, label='Reconstruction Errors', color='blue')

# Plot threshold line
plt.axvline(threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold: {threshold:.4f}')

# Plot mean line
plt.axvline(np.mean(reconstruction_errors), color='green', linestyle='-', linewidth=2, label=f'Mean: {np.mean(reconstruction_errors):.4f}')

plt.xlabel('Reconstruction Error', fontsize=12)
plt.ylabel('Frequency', fontsize=12)
plt.title('Distribution of Reconstruction Errors\n(Autoencoder Anomaly Detection)', fontsize=14)
plt.legend(fontsize=10)
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("[INFO] Histogram shows:")
print("       - Most images have low error (normal)")
print("       - Few images exceed threshold (anomalies)")

print("\n" + "="*80)
print("AUTOENCODER ANOMALY DETECTION COMPLETE!")
print("="*80)

print(f"\n[SUMMARY]")
print(f"  Dataset: Fashion MNIST ({len(x_test)} test images)")
print(f"  Architecture: Autoencoder (784 → {latent_dim} → 784)")
print(f"  Training: {len(x_train)} images, 10 epochs")
print(f"  Anomaly Detection: Threshold-based on reconstruction error")
print(f"  Threshold: {threshold:.6f}")
print(f"  Anomalies detected: {num_anomaly} images ({100*num_anomaly/len(x_test):.1f}%)")
```

---
'''
# THEORETICAL CONCEPTS USED IN CODE

## **1. AUTOENCODER ARCHITECTURE**

**What:** Neural network that learns compressed representation of input data

**Two Parts:**
- **Encoder:** Compresses input (784 features) → Bottleneck (64 features)
- **Decoder:** Decompresses bottleneck (64 features) → Output (784 features)

**Loss Function:**
- Reconstruction Error = ||Input - Output||²
- Goal: Minimize this error

**Why:**
- Unsupervised learning (no labels needed)
- Learns important features automatically
- Can detect anomalies by high reconstruction error

## **2. BOTTLENECK/LATENT REPRESENTATION**

**What:** Compressed middle layer with fewer neurons

**Purpose:**
- Forces network to learn essential features only
- Creates bottleneck that prevents information flow
- 784 → 64 is ~12× compression

**Result:**
- Normal data: Can reconstruct well (low error)
- Anomalies: Cannot reconstruct well (high error)

## **3. RECONSTRUCTION ERROR FOR ANOMALY DETECTION**

**Formula:** Error = Mean Squared Error between input and output

**Assumption:** 
- Autoencoder trained only on NORMAL data
- Learns to reconstruct normal patterns very well
- Cannot reconstruct anomalies (high error signals anomaly)

**Threshold:**
- Mean + 2×Std of training errors
- Based on normal distribution statistics
- ~95% of normal data within 2×std

## **4. NORMALIZATION**

**Why:** 
- Scale pixel values [0, 255] → [0, 1]
- Faster training, better numerical stability
- Neural networks prefer this scale

**Formula:** Normalized = Original / 255.0

---

# VIVA/ORAL EXAMINATION QUESTIONS

## **THEORY QUESTIONS**

**Q1: What is an autoencoder and how does it differ from a regular neural network?**
A: An autoencoder is an unsupervised neural network with two parts: encoder (compresses input) and decoder (reconstructs input). Unlike regular networks that learn to classify, autoencoders learn compressed representations. The input equals the target, and we minimize reconstruction error.

**Q2: What is the bottleneck in an autoencoder and why is it important?**
A: The bottleneck is the middle layer with fewer neurons (64 in our case). It forces the network to compress input information into a compact form. This compression makes the network learn only essential features, discarding noise. The bottleneck enables the autoencoder to detect anomalies.

**Q3: Why train autoencoder on normal data only?**
A: The autoencoder learns to reconstruct patterns from training data. If trained only on normal data, it becomes very good at reconstructing normal patterns. When tested on anomalies (never seen before), reconstruction fails (high error), signaling an anomaly.

**Q4: Explain the threshold mechanism for anomaly detection.**
A: Threshold = Mean(errors) + 2×Std(errors). If an image's reconstruction error exceeds this threshold, it's classified as anomaly. The threshold is based on assuming errors are normally distributed. ~95% of normal data falls within mean ±2×std.

**Q5: Why use sigmoid activation in the decoder's output layer?**
A: Sigmoid outputs values in [0, 1], matching normalized pixel values [0, 1]. This is crucial because: 1) Matches data range, 2) Provides smooth gradients, 3) Prevents values from exceeding valid range.

**Q6: What does reconstruction error measure?**
A: It measures how closely the autoencoder can reconstruct the input. Low error = good reconstruction (similar to training data). High error = poor reconstruction (different from training data). Anomalies typically have high error.

**Q7: Why normalize data to [0, 1]?**
A: Normalization: 1) Accelerates training (smaller gradients), 2) Improves numerical stability, 3) Prevents gradient explosion/vanishing, 4) Standardizes input scale, 5) Required for many activation functions.

**Q8: Difference between training loss and reconstruction error?**
A: Training loss: Average error over a batch during training. Reconstruction error: Individual error for each image. Training loss is computed during training optimization. Reconstruction error is computed after training for anomaly detection.

---

## **CODE-RELATED QUESTIONS**

**Q9: In Section E, why do we compile with x_train as both input and target?**
A: For autoencoders, input = target because we're training to reconstruct the same input. The network learns: Input → Encoder → Decoder → Output ≈ Input. Loss = ||Input - Output||².

**Q10: What happens if we use latent_dim=2 instead of 64?**
A: Too much compression → Information loss → Poor reconstruction. Even normal images would have high error. Cannot distinguish normal from anomaly.

**Q11: What if we use latent_dim=500?**
A: Too little compression → Network just memorizes inputs → No real learning. Reconstruction perfect for all data. Cannot detect anomalies (all errors low).

**Q12: Explain the layers.Flatten() in encoder.**
A: Converts 2D image (28, 28) to 1D vector (784). Dense layers need 1D input. Flattening preserves all pixel information while converting format.

**Q13: Why reshape in decoder from (None, 784) to (None, 28, 28)?**
A: Restores 2D spatial structure needed for visualization. Without reshape, output is 1D vector (uninterpretable as image). Reshape (None, 784) → (None, 28, 28).

**Q14: What does batch_size=256 mean in fit()?**
A: Process 256 images before one weight update. Larger batches: smoother gradients, fewer updates per epoch. Smaller batches: noisier gradients, more updates per epoch.

**Q15: Why use validation_data=(x_test, x_test) in fit()?**
A: Monitor reconstruction error on test set during training. Shows if model generalizes. Important for detecting overfitting (training error decreasing but validation error increasing).

---

## **DESIGN & MODIFICATION QUESTIONS**

**Q16: How would you detect multiple types of anomalies?**
A: Train separate autoencoders for each anomaly type, or train single autoencoder on diverse data. Use different thresholds for different anomaly types based on severity.

**Q17: What if reconstruction errors are not normally distributed?**
A: Use statistical tests (Shapiro-Wilk, Q-Q plot) to check. If not normal, use percentile-based threshold (e.g., 95th percentile) instead of mean+2×std.

**Q18: How to improve anomaly detection performance?**
A: 1) More training epochs, 2) Adjust latent_dim, 3) Change network architecture (more layers), 4) Use better threshold (ROC curve), 5) Data augmentation, 6) Ensemble methods.

**Q19: Why not use classification network to detect anomalies?**
A: Classification requires labeled anomaly data (expensive to obtain). Autoencoders need only normal data. Also, autoencoders can detect new anomaly types.

**Q20: How to handle imbalanced data (many normal, few anomalies)?**
A: Use class weights in loss function, or oversample anomalies, or change threshold. In threshold-based detection, can set threshold to catch desired percentage of data as anomalies.

---

## **KEY FORMULAS**

1. **Reconstruction Error:** Error = (1/n) × Σ(X - Autoencoder(X))²
2. **Threshold:** threshold = mean(errors) + 2 × std(errors)
3. **Anomaly Detection:** if error > threshold → Anomaly, else → Normal
'''

